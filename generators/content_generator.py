"""Claude AI ì½˜í…ì¸  ìƒì„±ê¸° - ì¹´í…Œê³ ë¦¬ë³„ ê³ í’ˆì§ˆ ë¸”ë¡œê·¸ ê¸€ ìƒì„±"""
import json
import logging
import re
import uuid
from typing import Optional, List
from dataclasses import dataclass, field
from pathlib import Path

import anthropic

try:
    import google.generativeai as genai
    HAS_GEMINI = True
except ImportError:
    HAS_GEMINI = False

# AI ë©”íƒ€ ì‘ë‹µ ì œê±° íŒ¨í„´
AI_META_PATTERNS = [
    r"^.*?ì œê³µí•´ì£¼ì‹ .*?ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤\.?\s*",
    r"^.*?ê°€ì´ë“œë¼ì¸.*?ë”°ë¼.*?ì‘ì„±.*?\s*",
    r"^.*?HTML ë¬¸ì„œë¥¼ ì‘ì„±.*?\s*",
    r"^.*?ì•„ë˜.*?êµ¬ì¡°ë¡œ.*?ì‘ì„±.*?\s*",
    r"^.*?ë§ì”€í•˜ì‹ .*?ëŒ€ë¡œ.*?\s*",
    r"^.*?ìš”ì²­í•˜ì‹ .*?ë‚´ìš©.*?\s*",
    r"^.*?ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•´.*?\s*",
    r"^.*?ë‹¤ìŒê³¼ ê°™ì´.*?ì‘ì„±.*?\s*",
]


def clean_ai_response(content: str) -> str:
    """
    AI ë©”íƒ€ ì‘ë‹µ ì œê±°

    Args:
        content: AIê°€ ìƒì„±í•œ ì½˜í…ì¸ 

    Returns:
        ì •ë¦¬ëœ ì½˜í…ì¸ 
    """
    # ë©”íƒ€ íŒ¨í„´ ì œê±°
    for pattern in AI_META_PATTERNS:
        content = re.sub(pattern, "", content, flags=re.DOTALL | re.MULTILINE)

    # HTML íƒœê·¸ë¡œ ì‹œì‘í•˜ë„ë¡ ì •ë¦¬
    # <div, <h2, <p ë“±ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ ì°¾ê¸°
    html_start = re.search(r'<(?:div|h[1-6]|p|section)', content, re.IGNORECASE)
    if html_start:
        content = content[html_start.start():]

    # ì•ë’¤ ê³µë°± ì •ë¦¬
    content = content.strip()

    return content


def clean_html_styles(html_content: str) -> str:
    """
    ë°œí–‰ ì „ ë¶ˆí•„ìš”í•œ ìŠ¤íƒ€ì¼ ì œê±° (ì™¼ìª½ ê²€ì€ ë¼ì¸ ë“±)

    WordPressì—ì„œ blockquote/border-left ìŠ¤íƒ€ì¼ì´
    ì™¼ìª½ ê²€ì€ ë¼ì¸ìœ¼ë¡œ í‘œì‹œë˜ëŠ” ë¬¸ì œ í•´ê²°

    Args:
        html_content: HTML ì½˜í…ì¸ 

    Returns:
        ì •ë¦¬ëœ HTML ì½˜í…ì¸ 
    """
    # blockquote íƒœê·¸ë¥¼ ì¼ë°˜ divë¡œ ë³€í™˜
    html_content = re.sub(
        r'<blockquote[^>]*>',
        '<div class="info-box">',
        html_content
    )
    html_content = html_content.replace('</blockquote>', '</div>')

    # border-left ì¸ë¼ì¸ ìŠ¤íƒ€ì¼ ì œê±° (ê°•ì¡° ë°•ìŠ¤ì˜ 4px solid ìƒ‰ìƒì€ ìœ ì§€)
    # ê°•ì¡° ë°•ìŠ¤ìš© border-left (4px solid #ìƒ‰ìƒ)ëŠ” ë³´ì¡´í•˜ê³  ë‚˜ë¨¸ì§€ë§Œ ì œê±°
    html_content = re.sub(
        r'border-left:\s*(?!4px\s+solid\s+#)[^;"]+;?',
        '',
        html_content,
        flags=re.IGNORECASE
    )

    # border-l-* Tailwind í´ë˜ìŠ¤ ì œê±°
    html_content = re.sub(
        r'\bborder-l-\d+\b',
        '',
        html_content
    )

    # border: ìŠ¤íƒ€ì¼ì—ì„œ ì™¼ìª½ë§Œ ìˆëŠ” ê²½ìš° ì œê±°
    html_content = re.sub(
        r'border:\s*\d+px\s+solid\s+[^;"]+\s*;\s*border-(?:right|top|bottom):\s*none\s*;',
        '',
        html_content,
        flags=re.IGNORECASE
    )

    # ë¹ˆ style ì†ì„± ì •ë¦¬
    html_content = re.sub(r'\s*style="\s*"', '', html_content)

    return html_content

import sys
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from config.settings import settings
from utils.image_fetcher import ImageFetcher
from utils.google_sheets import get_coupang_products
from utils.product_matcher import match_products_for_content, generate_product_html
from utils.web_search import GoogleSearcher
from utils.humanizer import humanize_full
from .prompts import (
    SYSTEM_PROMPT,
    STRUCTURE_PROMPT,
    get_title_prompt,
    CATEGORY_TEMPLATES,
    get_template,
    OFFICIAL_BUTTON_TEMPLATE,
    get_card_colors,
    COUPANG_BUTTON_TEMPLATE,
    COUPANG_DISCLAIMER,
    HEALTH_DISCLAIMER,
    AFFILIATE_NOTICE,
    CATEGORY_BADGE_TEMPLATE,
    PROFESSIONAL_PERSONA,
    post_process_content,
)
from .template_prompts import generate_template_prompt, get_template_info_log, PERSON_TITLE_PROMPT
from generators.humanizer import humanize_content
from media.link_matcher import insert_related_links

logger = logging.getLogger(__name__)

# ì„¤ì • íŒŒì¼ ê²½ë¡œ
CONFIG_DIR = Path(settings.config_dir)

# ì¿ íŒ¡ ì œì™¸ í‚¤ì›Œë“œ (ê¸ˆìœµ/íˆ¬ì ê´€ë ¨ - ì¹´í…Œê³ ë¦¬ ì„¤ì •ê³¼ ë¬´ê´€í•˜ê²Œ í•­ìƒ ì œì™¸)
COUPANG_EXCLUDE_KEYWORDS = [
    "ë¹„íŠ¸ì½”ì¸", "ì´ë”ë¦¬ì›€", "ì½”ì¸", "ì•”í˜¸í™”í", "ê°€ìƒí™”í", "ë¸”ë¡ì²´ì¸",
    "ì£¼ì‹", "íˆ¬ì", "í€ë“œ", "ETF", "ë°°ë‹¹", "ì¦ê¶Œ",
    "ëŒ€ì¶œ", "ê¸ˆë¦¬", "í™˜ìœ¨", "ì™¸í™˜",
    "ì—°ë§ì •ì‚°", "ì„¸ê¸ˆ", "í™˜ê¸‰",
]

# ì¿ íŒ¡ ì œì™¸ ì¹´í…Œê³ ë¦¬
COUPANG_EXCLUDE_CATEGORIES = ["ì—°ì˜ˆ", "íŠ¸ë Œë“œ", "ì¬í…Œí¬", "ì·¨ì—…êµìœ¡"]


def is_person_keyword(keyword: str) -> bool:
    """
    ì¸ë¬¼ í‚¤ì›Œë“œì¸ì§€ íŒë‹¨

    Args:
        keyword: ê²€ì‚¬í•  í‚¤ì›Œë“œ

    Returns:
        ì¸ë¬¼ í‚¤ì›Œë“œ ì—¬ë¶€
    """
    # 1. í•œê¸€ ì´ë¦„ íŒ¨í„´ (2-4ê¸€ì, ì„±+ì´ë¦„)
    korean_name_pattern = r'^[ê°€-í£]{2,4}$'
    if re.match(korean_name_pattern, keyword):
        # ì¼ë°˜ ëª…ì‚¬ ì œì™¸ (í‚¤ì›Œë“œê°€ ì¼ë°˜ ë‹¨ì–´ì¼ ê°€ëŠ¥ì„±)
        common_words = [
            "ì—°ë§ì •ì‚°", "ë¹„íŠ¸ì½”ì¸", "ì´ë”ë¦¬ì›€", "ì£¼ì‹", "ë¶€ë™ì‚°", "ì•„íŒŒíŠ¸",
            "ë‚ ì”¨", "í™˜ìœ¨", "ê¸ˆë¦¬", "ëŒ€ì¶œ", "ë³´í—˜", "ì—°ê¸ˆ", "ì²­ì•½",
            "ë‹¤ì´ì–´íŠ¸", "ê±´ê°•", "ìš´ë™", "ì—¬í–‰", "ë§›ì§‘", "ì¹´í˜",
            "ìë™ì°¨", "ìŠ¤ë§ˆíŠ¸í°", "ë…¸íŠ¸ë¶", "ê²Œì„", "ì˜í™”", "ë“œë¼ë§ˆ"
        ]
        if keyword in common_words:
            return False
        return True

    # 2. ì¸ë¬¼ ê´€ë ¨ ì§í•¨/ì§ì—… í¬í•¨ ì—¬ë¶€
    person_indicators = [
        "ì„ ìˆ˜", "ë°°ìš°", "ê°€ìˆ˜", "ì‚¬ì¥", "ëŒ€í‘œ", "ì˜ì›", "ì¥ê´€", "ê°ë…",
        "êµìˆ˜", "ì‘ê°€", "ê°ë…", "ì•„ë‚˜ìš´ì„œ", "MC", "ì½”ì¹˜", "íšŒì¥",
        "ì´ì¥", "ì›ì¥", "ì†Œì¥", "ë¶€ì¥", "ì°¨ì¥", "ê³¼ì¥"
    ]
    for indicator in person_indicators:
        if indicator in keyword:
            return True

    # 3. ìœ ëª…ì¸ ì´ë¦„ íŒ¨í„´ (ì˜ë¬¸ í¬í•¨)
    celebrity_patterns = [
        # ì—°ì˜ˆì¸
        "ì†í¥ë¯¼", "BTS", "ë¸”ë™í•‘í¬", "ë‰´ì§„ìŠ¤", "ì•„ì´ë¸Œ", "ì—ìŠ¤íŒŒ",
        "ë°•ë³´ê²€", "ì†¡í˜œêµ", "ê¹€ìˆ˜í˜„", "ì´ë¯¼í˜¸", "ì „ì§€í˜„", "ìˆ˜ì§€",
        "ì•„ì´ìœ ", "ì„ì˜ì›…", "ë°•ì„œì¤€", "ì •í•´ì¸", "ì°¨ì€ìš°",
        # ìŠ¤í¬ì¸ 
        "ì´ê°•ì¸", "ê¹€ë¯¼ì¬", "í™©í¬ì°¬", "ì˜¤íƒ€ë‹ˆ",
        # ì •ì¹˜ì¸
        "ìœ¤ì„ì—´", "ì´ì¬ëª…", "í•œë™í›ˆ",
    ]
    for celeb in celebrity_patterns:
        if celeb in keyword or keyword in celeb:
            return True

    return False


@dataclass
class Section:
    """ì„¹ì…˜ ë°ì´í„°"""
    id: str
    index: int
    type: str  # heading, image, paragraph, list, table, quote
    html: str


@dataclass
class GeneratedPost:
    """ìƒì„±ëœ í¬ìŠ¤íŠ¸ ë°ì´í„°"""
    title: str
    content: str
    excerpt: str
    category: str
    template: str
    has_coupang: bool = False
    sources: list = None  # ì›¹ê²€ìƒ‰ ì¶œì²˜ ëª©ë¡
    sections: List[Section] = field(default_factory=list)  # ì„¹ì…˜ ë°°ì—´

    def __post_init__(self):
        if self.sources is None:
            self.sources = []
        if self.sections is None:
            self.sections = []

    def to_dict(self) -> dict:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (API ì‘ë‹µìš©)"""
        return {
            "title": self.title,
            "content": self.content,
            "excerpt": self.excerpt,
            "category": self.category,
            "template": self.template,
            "has_coupang": self.has_coupang,
            "sources": self.sources,
            "sections": [
                {"id": s.id, "index": s.index, "type": s.type, "html": s.html}
                for s in self.sections
            ]
        }


class ContentGenerator:
    """Claude AIë¥¼ ì‚¬ìš©í•œ ì¹´í…Œê³ ë¦¬ë³„ ê³ í’ˆì§ˆ ì½˜í…ì¸  ìƒì„±ê¸°"""

    def __init__(self):
        self.ai_provider = settings.ai_provider.lower()
        self.client = anthropic.Anthropic(api_key=settings.claude_api_key)
        self.model = settings.claude_model

        # Gemini ì´ˆê¸°í™”
        if self.ai_provider == "gemini" and HAS_GEMINI:
            if settings.gemini_api_key:
                genai.configure(api_key=settings.gemini_api_key)
                self.gemini_model = genai.GenerativeModel(settings.gemini_model)
                logger.info(f"AI Provider: Gemini ({settings.gemini_model})")
            else:
                logger.warning("Gemini selected but GOOGLE_API_KEY not set, falling back to Claude")
                self.ai_provider = "claude"
        elif self.ai_provider == "gemini" and not HAS_GEMINI:
            logger.warning("google-generativeai not installed, falling back to Claude")
            self.ai_provider = "claude"

        if self.ai_provider == "claude":
            logger.info(f"AI Provider: Claude ({self.model})")
        self.coupang_id = settings.coupang_partner_id
        self.image_fetcher = ImageFetcher()
        self.web_searcher = GoogleSearcher()

        # íŠ¸ë Œë“œ ë§¥ë½ ìˆ˜ì§‘ìš© í¬ë¡¤ëŸ¬
        from crawlers.google_trends import GoogleTrendsCrawler
        self.trend_crawler = GoogleTrendsCrawler()

        # ì„¤ì • íŒŒì¼ ë¡œë“œ
        self.categories_config = self._load_json("categories.json")
        self.official_links = self._load_json("official_links.json")
        self.coupang_links = self._load_json("coupang_links.json")
        self.coupang_defaults = self._load_json("coupang_defaults.json")
        self.evergreen_config = self._load_json("evergreen_keywords.json")

    def _load_json(self, filename: str) -> dict:
        """JSON ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            filepath = CONFIG_DIR / filename
            with open(filepath, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.warning(f"Config file not found: {filename}")
            return {}
        except json.JSONDecodeError as e:
            logger.error(f"Error parsing {filename}: {e}")
            return {}

    def is_evergreen_keyword(self, keyword: str) -> bool:
        """
        ì—ë²„ê·¸ë¦° í‚¤ì›Œë“œì¸ì§€ í™•ì¸

        Args:
            keyword: ê²€ì‚¬í•  í‚¤ì›Œë“œ

        Returns:
            ì—ë²„ê·¸ë¦° í‚¤ì›Œë“œ ì—¬ë¶€
        """
        detection_keywords = self.evergreen_config.get("detection_keywords", [])

        for eg_keyword in detection_keywords:
            if eg_keyword.lower() in keyword.lower():
                logger.info(f"Evergreen keyword detected: '{keyword}' matches '{eg_keyword}'")
                return True

        return False

    def get_trend_context(self, keyword: str) -> str:
        """
        í‚¤ì›Œë“œì˜ íŠ¸ë Œë“œ ë§¥ë½ ìˆ˜ì§‘ (ì™œ ì§€ê¸ˆ ì´ í‚¤ì›Œë“œê°€ í™”ì œì¸ì§€)

        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ

        Returns:
            íŠ¸ë Œë“œ ë§¥ë½ ë¬¸ìì—´ (í”„ë¡¬í”„íŠ¸ì— ì‚½ì…ìš©)
        """
        try:
            context = self.trend_crawler.get_trend_context(keyword)

            if context.get("news_titles"):
                news_list = '\n'.join([f'- {title}' for title in context['news_titles'][:5]])
                return f"""
[íŠ¸ë Œë“œ ë°°ê²½ - ì´ í‚¤ì›Œë“œê°€ ì§€ê¸ˆ í™”ì œì¸ ì´ìœ ]
{news_list}

ì¤‘ìš”: ìœ„ ë‰´ìŠ¤ ë‚´ìš©ì„ ë°˜ì˜í•˜ì—¬ "ì™œ ì§€ê¸ˆ ì´ í‚¤ì›Œë“œê°€ ëœ¨ëŠ”ì§€" ì„¤ëª…í•´ì£¼ì„¸ìš”.
ë‹¨ìˆœí•œ ì¼ë°˜ë¡ ì´ ì•„ë‹Œ, í˜„ì¬ ìƒí™©ì— ë§ëŠ” ì‹œì˜ì„± ìˆëŠ” ë‚´ìš©ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
            return ""

        except Exception as e:
            logger.warning(f"íŠ¸ë Œë“œ ë§¥ë½ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return ""

    def _call_claude(
        self,
        user_prompt: str,
        system_prompt: str = SYSTEM_PROMPT,
        max_tokens: int = 8000,
        use_persona: bool = True
    ) -> str:
        """
        Claude API í˜¸ì¶œ

        Args:
            user_prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            use_persona: ì „ë¬¸ê°€ í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© ì—¬ë¶€

        Returns:
            Claude ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        try:
            # ì „ë¬¸ê°€ í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ì¶”ê°€ (AdSense ìµœì í™”)
            if use_persona:
                full_system_prompt = PROFESSIONAL_PERSONA + "\n\n" + system_prompt
            else:
                full_system_prompt = system_prompt

            message = self.client.messages.create(
                model=self.model,
                max_tokens=max_tokens,
                system=full_system_prompt,
                messages=[
                    {"role": "user", "content": user_prompt}
                ]
            )
            return message.content[0].text

        except anthropic.APIError as e:
            logger.error(f"Claude API error: {e}")
            raise
        except Exception as e:
            logger.error(f"Error calling Claude: {e}")
            raise

    def _call_gemini(
        self,
        user_prompt: str,
        system_prompt: str = SYSTEM_PROMPT,
        max_tokens: int = 8000,
        use_persona: bool = True
    ) -> str:
        """Gemini API í˜¸ì¶œ"""
        try:
            if use_persona:
                full_prompt = PROFESSIONAL_PERSONA + "\n\n" + system_prompt + "\n\n" + user_prompt
            else:
                full_prompt = system_prompt + "\n\n" + user_prompt

            response = self.gemini_model.generate_content(
                full_prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=max_tokens,
                    temperature=0.7,
                )
            )
            return response.text

        except Exception as e:
            logger.error(f"Gemini API error: {e}")
            raise

    def _call_ai(
        self,
        user_prompt: str,
        system_prompt: str = SYSTEM_PROMPT,
        max_tokens: int = 8000,
        use_persona: bool = True
    ) -> str:
        """í†µí•© AI í˜¸ì¶œ - ai_provider ì„¤ì •ì— ë”°ë¼ Claude/Gemini ì„ íƒ"""
        if self.ai_provider == "gemini":
            return self._call_gemini(user_prompt, system_prompt, max_tokens, use_persona)
        return self._call_claude(user_prompt, system_prompt, max_tokens, use_persona)

    def classify_category(self, keyword: str) -> tuple[str, dict]:
        """
        í‚¤ì›Œë“œ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜

        Returns:
            (ì¹´í…Œê³ ë¦¬ëª…, ì¹´í…Œê³ ë¦¬ ì„¤ì •) íŠœí”Œ
        """
        categories = self.categories_config.get("categories", {})

        best_match = "íŠ¸ë Œë“œ"
        best_priority = 999
        best_config = categories.get("íŠ¸ë Œë“œ", {})

        for category_name, config in categories.items():
            if config.get("is_default"):
                continue

            keywords = config.get("keywords", [])
            priority = config.get("priority", 99)

            for kw in keywords:
                if kw.lower() in keyword.lower() and priority < best_priority:
                    best_match = category_name
                    best_priority = priority
                    best_config = config
                    break

        logger.info(f"Category classified: '{keyword}' -> {best_match} (template: {best_config.get('template', 'trend')})")
        return best_match, best_config

    def generate_title(self, keyword: str, news_data: str = "", is_person: bool = False) -> str:
        """
        ë¸”ë¡œê·¸ ì œëª© ìƒì„±

        Args:
            keyword: í‚¤ì›Œë“œ
            news_data: ë‰´ìŠ¤/ì›¹ê²€ìƒ‰ ë°ì´í„° (ì¸ë¬¼ í‚¤ì›Œë“œìš©)
            is_person: ì¸ë¬¼ í‚¤ì›Œë“œ ì—¬ë¶€

        Returns:
            ìƒì„±ëœ ì œëª©
        """
        if is_person and news_data:
            # ì¸ë¬¼ ì „ìš© ì œëª© í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            prompt = PERSON_TITLE_PROMPT.format(
                keyword=keyword,
                news_summary=news_data[:800]  # ë‰´ìŠ¤ ìš”ì•½ ì „ë‹¬
            )
            logger.info(f"Using person title prompt for: {keyword}")
        else:
            # ê¸°ì¡´ ì œëª© í”„ë¡¬í”„íŠ¸
            prompt = get_title_prompt(keyword)

        # ì œëª© ìƒì„±ì—ëŠ” í˜ë¥´ì†Œë‚˜ ë¯¸ì‚¬ìš©
        title = self._call_ai(prompt, max_tokens=200, use_persona=False)
        title = title.strip().strip('"\'')
        
        # Gemini ì œëª© ì˜ë¦¼ ë³´ì •: ì œëª©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ í‚¤ì›Œë“œ ê¸°ë°˜ ì¬ìƒì„±
        if len(title) < 15:
            logger.warning(f"Title too short ({len(title)} chars): '{title}', regenerating...")
            fallback_prompt = f"'{keyword}'ì— ëŒ€í•œ ë¸”ë¡œê·¸ ê¸€ ì œëª©ì„ 40ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”. ì œëª©ë§Œ í•œ ì¤„ë¡œ ì¶œë ¥í•˜ì„¸ìš”."
            title = self._call_ai(fallback_prompt, max_tokens=100, use_persona=False)
            title = title.strip().strip('"\'')
            # ì—¬ì „íˆ ì§§ìœ¼ë©´ í‚¤ì›Œë“œ ì§ì ‘ í™œìš©
            if len(title) < 15:
                title = f"{keyword}, ê¼­ ì•Œì•„ì•¼ í•  í•µì‹¬ ì •ë³´"
        
        # ì œëª© ì¤‘ë³µ ë‹¨ì–´ ì œê±° (ì˜ˆ: "ì´ì •ë¦¬ ì´ì •ë¦¬" â†’ "ì´ì •ë¦¬")
        import re as _re
        title = _re.sub(r'(\S+)\s+\1', r'\1', title)
        
        return title

    def perform_web_search(self, keyword: str) -> dict:
        """
        íŠ¸ë Œë“œ í‚¤ì›Œë“œì— ëŒ€í•´ ì›¹ê²€ìƒ‰ ìˆ˜í–‰

        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ

        Returns:
            ì›¹ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.web_searcher.is_configured():
            logger.warning("Google Search API not configured - skipping web search")
            return {"keyword": keyword, "sources": [], "content": ""}

        logger.info(f"Performing web search for: {keyword}")
        result = self.web_searcher.search_and_crawl(keyword, num_results=5)

        if result.get("sources"):
            logger.info(f"  Found {len(result['sources'])} sources")
            for src in result['sources'][:3]:
                logger.info(f"    - {src['title'][:40]}...")
        else:
            logger.warning(f"  No web search results found")

        return result

    def generate_content_with_template(
        self,
        keyword: str,
        news_data: str,
        template_name: str,
        category_name: str = "íŠ¸ë Œë“œ",
        is_evergreen: bool = False,
        web_data: dict = None,
        trend_context: str = ""
    ) -> tuple[str, list, dict]:
        """
        í…œí”Œë¦¿ ë‹¤ì–‘í™” ì‹œìŠ¤í…œìœ¼ë¡œ ë³¸ë¬¸ ìƒì„± (ì €í’ˆì§ˆ ë°©ì§€)

        Args:
            keyword: í‚¤ì›Œë“œ
            news_data: ë‰´ìŠ¤ ë°ì´í„°
            template_name: ê¸°ì¡´ í…œí”Œë¦¿ ì´ë¦„ (í˜¸í™˜ìš©, ì‹¤ì œ ì‚¬ìš© ì•ˆ í•¨)
            category_name: ì¹´í…Œê³ ë¦¬ëª…
            is_evergreen: ì—ë²„ê·¸ë¦° ì½˜í…ì¸  ì—¬ë¶€
            web_data: ì›¹ê²€ìƒ‰ ê²°ê³¼ (íŠ¸ë Œë“œ í‚¤ì›Œë“œìš©)
            trend_context: íŠ¸ë Œë“œ ë§¥ë½ (ì™œ ì§€ê¸ˆ ì´ í‚¤ì›Œë“œê°€ í™”ì œì¸ì§€)

        Returns:
            (HTML ë³¸ë¬¸, ì¶œì²˜ ëª©ë¡, í…œí”Œë¦¿ ì •ë³´) íŠœí”Œ
        """
        # ì›¹ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì°¸ê³  ìë£Œë¡œ ì¶”ê°€
        sources = []
        web_data_content = ""

        # íŠ¸ë Œë“œ ë§¥ë½ì´ ìˆìœ¼ë©´ ë¨¼ì € ì¶”ê°€ (ì‹œì˜ì„± ìˆëŠ” ê¸€ ì‘ì„±ì„ ìœ„í•´)
        if trend_context:
            web_data_content = trend_context + "\n\n"
            logger.info("Added trend context to prompt")

        if web_data and web_data.get("content"):
            web_content = web_data["content"][:6000]  # í† í° ì œí•œ ê³ ë ¤
            sources = web_data.get("sources", [])

            web_data_content += f"""
[ì›¹ê²€ìƒ‰ ê²°ê³¼ - ìµœì‹  ì •ë³´ (ë°˜ë“œì‹œ ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±)]
{web_content}

[ê¸°ì¡´ ë‰´ìŠ¤ ë°ì´í„°]
{news_data}

[ì¤‘ìš” ê·œì¹™]
1. ìœ„ ì°¸ê³  ìë£Œì˜ íŒ©íŠ¸ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
2. ìë£Œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
3. ìµœì‹  ë‚ ì§œ, ê¸ˆì•¡, ìˆ˜ì¹˜ë¥¼ ì •í™•íˆ ë°˜ì˜í•˜ì„¸ìš”
4. íŠ¸ë Œë“œ ë°°ê²½ì´ ìˆë‹¤ë©´ "ì™œ ì§€ê¸ˆ ì´ í‚¤ì›Œë“œê°€ í™”ì œì¸ì§€" ê¼­ ì–¸ê¸‰í•˜ì„¸ìš”
"""
            logger.info(f"Added web search data: {len(web_content)} chars from {len(sources)} sources")
        elif news_data:
            web_data_content += news_data

        # ì¸ë¬¼ í‚¤ì›Œë“œ ì—¬ë¶€ í™•ì¸
        is_person = is_person_keyword(keyword)
        if is_person:
            logger.info(f"Person keyword detected: {keyword}")

        # ğŸ†• í…œí”Œë¦¿ ë‹¤ì–‘í™” ì‹œìŠ¤í…œ ì‚¬ìš© (ì €í’ˆì§ˆ ë°©ì§€)
        prompt, template_key, template, cta_config = generate_template_prompt(
            keyword=keyword,
            category=category_name,
            web_data=web_data_content,
            is_evergreen=is_evergreen,
            is_person=is_person
        )

        # í…œí”Œë¦¿ ì •ë³´ ë¡œê¹…
        template_info = get_template_info_log(template_key, template, cta_config)
        print(template_info)
        logger.info(f"Template selected: {template_key} ({template['name']})")
        logger.info(f"Target words: {template['selected_word_count']}, Images: {template['selected_image_count']}")

        # ëª¨ë¸ ì œí•œ ë‚´ì—ì„œ ìµœëŒ€ì¹˜ ì‚¬ìš© (Haiku: 8192)
        max_tokens = 8000

        content = self._call_ai(prompt, max_tokens=max_tokens)

        # HTML ì½”ë“œ ë¸”ë¡ ì œê±°
        content = re.sub(r'^```html\s*', '', content, flags=re.MULTILINE)
        content = re.sub(r'\s*```$', '', content, flags=re.MULTILINE)

        # AI ë©”íƒ€ ì‘ë‹µ ì œê±°
        content = clean_ai_response(content)

        # ë³¸ë¬¸ ì‹œì‘ì˜ ëŒ€ì œëª© ì œê±° (WP í…Œë§ˆê°€ ë³„ë„ë¡œ ì œëª© í‘œì‹œí•˜ë¯€ë¡œ ì¤‘ë³µ)
        content = re.sub(r'^\s*<h2[^>]*style="[^"]*text-align:\s*center[^"]*"[^>]*>.*?</h2>\s*', '', content, count=1, flags=re.DOTALL)

        # placeholder ì´ë¯¸ì§€ ì œê±°
        content = re.sub(r'<p[^>]*>\s*<img[^>]*src="https://via\.placeholder\.com[^"]*"[^>]*/?\s*>\s*</p>', '', content, flags=re.DOTALL)
        content = re.sub(r'<img[^>]*src="https://via\.placeholder\.com[^"]*"[^>]*/?\s*>', '', content, flags=re.DOTALL)

        # ë‚¨ì€ IMAGE íƒœê·¸ ë° IMG_CONTEXT ì£¼ì„ ì œê±°
        content = re.sub(r'<!-- IMG_CONTEXT: .+? -->\s*', '', content)
        content = re.sub(r'\[IMAGE_\d+[^\]]*\]', '', content)

        # AdSense ìµœì í™” í›„ì²˜ë¦¬ (ê¸ˆì§€ í‘œí˜„ ì œê±°, ì´ëª¨ì§€ ì œí•œ)
        content = post_process_content(content)

        # ì¸ê°„í™” ì²˜ë¦¬ (AI íƒì§€ íšŒí”¼)
        content = humanize_full(content, keyword)

        # í…œí”Œë¦¿ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
        template_info_dict = {
            "key": template_key,
            "name": template["name"],
            "word_count": template["selected_word_count"],
            "image_count": template["selected_image_count"],
            "cta_position": cta_config["position"]
        }

        return content.strip(), sources, template_info_dict

    def _extract_meta_description(self, content: str) -> str:
        """ë©”íƒ€ ì„¤ëª… ì¶”ì¶œ"""
        match = re.search(r'\[META\](.*?)\[/META\]', content, flags=re.DOTALL)
        if match:
            return match.group(1).strip()[:160]
        return ""

    def get_official_link(self, keyword: str) -> Optional[dict]:
        """ê³µì‹ ì‚¬ì´íŠ¸ ë§í¬ ì°¾ê¸°"""
        for key, info in self.official_links.items():
            if key in keyword:
                return info
        return None

    def get_coupang_link(self, keyword: str) -> Optional[dict]:
        """ì¿ íŒ¡ ë§í¬ ì°¾ê¸° (ì¹´í…Œê³ ë¦¬ë³„)"""
        for category, info in self.coupang_links.items():
            keywords = info.get("keywords", [])
            for kw in keywords:
                if kw in keyword:
                    return {
                        "url": f"{info['url']}?lptag={self.coupang_id}",
                        "button_text": info.get("button_text", "ì¿ íŒ¡ì—ì„œ í™•ì¸í•˜ê¸°")
                    }
        return None

    def should_exclude_coupang(self, keyword: str, category_name: str) -> bool:
        """
        ì¿ íŒ¡ ë°°ë„ˆ ì œì™¸ ì—¬ë¶€ íŒë‹¨

        Args:
            keyword: í‚¤ì›Œë“œ
            category_name: ì¹´í…Œê³ ë¦¬ëª…

        Returns:
            Trueë©´ ì¿ íŒ¡ ì œì™¸
        """
        # 1. ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ì œì™¸
        if category_name in COUPANG_EXCLUDE_CATEGORIES:
            logger.info(f"Coupang excluded: category '{category_name}' in exclude list")
            return True

        # 2. í‚¤ì›Œë“œ ê¸°ë°˜ ì œì™¸ (ê¸ˆìœµ/íˆ¬ì ê´€ë ¨)
        keyword_lower = keyword.lower()
        for exclude_kw in COUPANG_EXCLUDE_KEYWORDS:
            if exclude_kw.lower() in keyword_lower:
                logger.info(f"Coupang excluded: keyword '{exclude_kw}' found in '{keyword}'")
                return True

        return False

    def insert_images(
        self,
        content: str,
        keyword: str,
        category_name: str,
        count: int = 2,
        use_mixed: bool = True
    ) -> str:
        """
        í˜¼í•© ì´ë¯¸ì§€ ì‹œìŠ¤í…œìœ¼ë¡œ [IMAGE_N] íƒœê·¸ë¥¼ ì‹¤ì œ ì´ë¯¸ì§€ë¡œ êµì²´

        Phase 3: Puppeteer ìŠ¤í¬ë¦°ìƒ· + Pexels í˜¼í•©

        1. ìŠ¤í¬ë¦°ìƒ· ì¡°ê±´ ì¶©ì¡± ì‹œ ì²« ì´ë¯¸ì§€ë¥¼ ìŠ¤í¬ë¦°ìƒ·ìœ¼ë¡œ
        2. ë‚˜ë¨¸ì§€ëŠ” AI ê¸°ë°˜ Pexels ê²€ìƒ‰

        Args:
            content: HTML ë³¸ë¬¸
            keyword: í‚¤ì›Œë“œ
            category_name: ì¹´í…Œê³ ë¦¬ ì´ë¦„
            count: í•„ìš”í•œ ì´ë¯¸ì§€ ê°œìˆ˜
            use_mixed: í˜¼í•© ì´ë¯¸ì§€ ì‹œìŠ¤í…œ ì‚¬ìš© ì—¬ë¶€

        Returns:
            ì´ë¯¸ì§€ê°€ ì‚½ì…ëœ HTML
        """
        images = {}

        try:
            # í˜¼í•© ì´ë¯¸ì§€ ì‹œìŠ¤í…œ ì‚¬ìš© (Phase 3)
            if use_mixed:
                logger.info(f"Fetching mixed images for '{keyword}' (count: {count})")
                images = self.image_fetcher.fetch_mixed_images(
                    content, keyword, category_name, count
                )
            else:
                # í´ë°±: ê¸°ì¡´ AI ê¸°ë°˜ Pexelsë§Œ ì‚¬ìš©
                logger.info(f"Fetching contextual images for '{keyword}'")
                images = self.image_fetcher.fetch_contextual_images(content, keyword)
        except Exception as e:
            logger.error(f"Image fetching failed for '{keyword}': {e}")
            # ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨í•´ë„ ê¸€ ë°œí–‰ì€ ê³„ì† ì§„í–‰
            images = {}

        if not images:
            logger.warning(f"No images found for {keyword}, continuing without images")
            # ì´ë¯¸ì§€ íƒœê·¸ ë° IMG_CONTEXT ì£¼ì„ ì œê±° (í™•ì¥ íŒ¨í„´: ì½œë¡  í¬í•¨)
            content = re.sub(r'<!-- IMG_CONTEXT: .+? -->\s*', '', content)
            content = re.sub(r'\[IMAGE_\d+[^\]]*\]', '', content)  # [IMAGE_N: ì„¤ëª…] í¬í•¨
            return content

        # ê° ì´ë¯¸ì§€ íƒœê·¸ë¥¼ ì‹¤ì œ ì´ë¯¸ì§€ë¡œ êµì²´
        for tag, img_data in images.items():
            # URL ìœ íš¨ì„± í™•ì¸
            if not img_data.get('url') or not img_data['url'].startswith('http'):
                logger.warning(f"Invalid image URL for {tag}: {img_data.get('url')}")
                continue

            # ìº¡ì…˜: ì£¼ì œ ê´€ë ¨ ì„¤ëª… (Pexels ì¶œì²˜ ì œê±°)
            caption = img_data.get('alt', keyword)

            img_html = f'''
<figure style="text-align: center; margin: 30px 0;">
    <img src="{img_data['url']}"
         alt="{img_data['alt']}"
         style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);"
         loading="lazy" />
    <figcaption style="margin-top: 10px; color: #666; font-size: 14px;">
        {caption}
    </figcaption>
</figure>
'''
            # tagì—ì„œ ìˆ«ì ì¶”ì¶œ (IMAGE_1 -> 1)
            tag_num = tag.replace("IMAGE_", "")

            # íŒ¨í„´ 1: IMG_CONTEXT ì£¼ì„ + ì´ë¯¸ì§€ íƒœê·¸ (ì½œë¡  í¬í•¨)
            pattern1 = rf'<!-- IMG_CONTEXT: .+? -->\s*\[IMAGE_{tag_num}[^\]]*\]'

            # íŒ¨í„´ 2: ì´ë¯¸ì§€ íƒœê·¸ë§Œ (ì½œë¡  í¬í•¨) - [IMAGE_1: ì„¤ëª…] ë˜ëŠ” [IMAGE_1]
            pattern2 = rf'\[IMAGE_{tag_num}[^\]]*\]'

            if re.search(pattern1, content):
                content = re.sub(pattern1, img_html, content, count=1)
                logger.info(f"Inserted {tag} (with context): {img_data['search_query']}")
            elif re.search(pattern2, content):
                content = re.sub(pattern2, img_html, content, count=1)
                logger.info(f"Inserted {tag} (tag only): {img_data['search_query']}")
            else:
                logger.warning(f"Tag {tag} not found in content, inserting at section break")
                # íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì ì ˆí•œ h3/h2 í—¤ë” ë’¤ì— ì‚½ì…
                headers = list(re.finditer(r'</h[23]>', content))
                tag_idx = int(tag_num) if tag_num.isdigit() else 1
                # tag_idxë²ˆì§¸ í—¤ë” ë’¤ì— ì‚½ì… (ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ í—¤ë” ë’¤)
                if headers:
                    insert_after = min(tag_idx, len(headers)) - 1
                    pos = headers[insert_after].end()
                    # ë°”ë¡œ ë’¤ì— <p> ìˆìœ¼ë©´ ê·¸ ë¬¸ë‹¨ ë’¤ì— ì‚½ì…
                    next_p = re.search(r'</p>', content[pos:])
                    if next_p:
                        pos = pos + next_p.end()
                    content = content[:pos] + img_html + content[pos:]
                    logger.info(f"Force-inserted {tag} after header #{insert_after+1}")

        # ë‚¨ì€ IMG_CONTEXT ì£¼ì„ ë° ì´ë¯¸ì§€ íƒœê·¸ ì œê±° (í™•ì¥ íŒ¨í„´)
        content = re.sub(r'<!-- IMG_CONTEXT: .+? -->\s*', '', content)
        content = re.sub(r'\[IMAGE_\d+[^\]]*\]', '', content)  # [IMAGE_N: ì„¤ëª…] í¬í•¨

        return content

    def insert_official_link(self, content: str, keyword: str) -> str:
        """[OFFICIAL_LINK] íƒœê·¸ë¥¼ ì¹´ë“œí˜• ê³µì‹ ì‚¬ì´íŠ¸ ë§í¬ë¡œ êµì²´"""
        official = self.get_official_link(keyword)

        if official:
            url = official["url"]
            name = official["name"]
            description = official.get("description", f"{name} ê³µì‹ í™ˆí˜ì´ì§€")
            # íŒŒë¹„ì½˜ URL ìƒì„±
            from urllib.parse import urlparse
            domain = urlparse(url).netloc
            favicon_url = f"https://www.google.com/s2/favicons?domain={domain}&sz=64"
            
            button_html = OFFICIAL_BUTTON_TEMPLATE.format(
                url=url,
                name=name,
                description=description,
                favicon_url=favicon_url
            )
            content = content.replace("[OFFICIAL_LINK]", button_html)
            logger.info(f"Official card link inserted: {name}")
        else:
            content = content.replace("[OFFICIAL_LINK]", "")

        return content

    def insert_disclaimer(self, content: str) -> str:
        """[DISCLAIMER] íƒœê·¸ë¥¼ ê±´ê°• ë©´ì±…ë¬¸êµ¬ë¡œ êµì²´"""
        content = content.replace("[DISCLAIMER]", HEALTH_DISCLAIMER)
        return content

    def insert_affiliate_notice(self, content: str, has_coupang: bool = False) -> str:
        """
        [AFFILIATE_NOTICE] íƒœê·¸ë¥¼ íŒŒíŠ¸ë„ˆìŠ¤ ë¬¸êµ¬ë¡œ êµì²´

        í•µì‹¬ ë¡œì§: ì¿ íŒ¡ ë°°ë„ˆê°€ ìˆì„ ë•Œë§Œ íŒŒíŠ¸ë„ˆìŠ¤ ë¬¸êµ¬ ì‚½ì…
        """
        # íƒœê·¸ ì œê±° (ì—´ê¸°/ë‹«ê¸° ëª¨ë‘)
        content = content.replace("[AFFILIATE_NOTICE]", "")
        content = content.replace("[/AFFILIATE_NOTICE]", "")

        if has_coupang:
            # ì¿ íŒ¡ ë°°ë„ˆê°€ ìˆê³ , ì•„ì§ ë¬¸êµ¬ê°€ ì—†ì„ ë•Œë§Œ ì¶”ê°€
            if "ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ í™œë™" not in content:
                content += AFFILIATE_NOTICE
                logger.info("Affiliate notice inserted (coupang exists)")
        else:
            # ì¿ íŒ¡ ë°°ë„ˆê°€ ì—†ìœ¼ë©´ Claudeê°€ ìì²´ ìƒì„±í•œ íŒŒíŠ¸ë„ˆìŠ¤ ë¬¸êµ¬ë„ ì œê±°
            patterns_to_remove = [
                r'<p[^>]*>.*?ì´ í¬ìŠ¤íŒ…ì€ íŒŒíŠ¸ë„ˆì‹­ ë° ê´‘ê³  í¬í•¨ ì½˜í…ì¸ .*?</p>',
                r'<p[^>]*>.*?ì´ í¬ìŠ¤íŒ…ì€ ì œíœ´ ë§ˆì¼€íŒ… í™œë™ì˜ ì¼í™˜ìœ¼ë¡œ.*?</p>',
                r'<p[^>]*>.*?ì´ í¬ìŠ¤íŒ…ì€ ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ í™œë™ì˜ ì¼í™˜ìœ¼ë¡œ.*?</p>',
                r'<p[^>]*>.*?íŒŒíŠ¸ë„ˆì‹­ ë§í¬ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆ.*?</p>',
                r'<p[^>]*>.*?ì´ ê¸€ì—ëŠ” ì œíœ´ ë§í¬ê°€ í¬í•¨.*?</p>',
                r'ì´ í¬ìŠ¤íŒ…ì€ íŒŒíŠ¸ë„ˆì‹­ ë° ê´‘ê³  í¬í•¨ ì½˜í…ì¸ ì´ì—ìš”\.?',
                r'ì´ í¬ìŠ¤íŒ…ì€ ì œíœ´ ë§ˆì¼€íŒ… í™œë™ì˜ ì¼í™˜ìœ¼ë¡œ.*?ì‘ì„±ë˜ì—ˆì–´ìš”\.?',
                r': ì´ ê¸€ì—ëŠ” íŒŒíŠ¸ë„ˆì‹­ ë§í¬ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆì–´ìš”\.?',
            ]

            for pattern in patterns_to_remove:
                content = re.sub(pattern, '', content, flags=re.IGNORECASE | re.DOTALL)

            logger.info("Affiliate notice skipped and cleaned (no coupang)")

        return content

    def insert_category_badge(self, content: str, category_name: str) -> str:
        """ê¸€ ìƒë‹¨ì— ì¹´í…Œê³ ë¦¬ ë±ƒì§€ ì‚½ì…"""
        badge = CATEGORY_BADGE_TEMPLATE.format(category=category_name)

        # <div style="text-align: center; ë°”ë¡œ ë’¤ì— ì‚½ì…
        if '<div style="text-align: center;' in content:
            content = content.replace(
                '<div style="text-align: center; line-height: 2.0;">',
                f'<div style="text-align: center; line-height: 2.0;">\n{badge}',
                1  # ì²« ë²ˆì§¸ë§Œ êµì²´
            )
        else:
            # ì—†ìœ¼ë©´ ë§¨ ì•ì— ì¶”ê°€
            content = badge + content

        return content

    def insert_coupang_products(
        self,
        content: str,
        keyword: str,
        category_config: dict,
        category_name: str = ""
    ) -> tuple[str, bool]:
        """
        [COUPANG] íƒœê·¸ë¥¼ ì¿ íŒ¡ ìƒí’ˆìœ¼ë¡œ êµì²´

        ì œì™¸ ì¡°ê±´ (ìš°ì„  ì ìš©):
        - ì¹´í…Œê³ ë¦¬: ì—°ì˜ˆ, íŠ¸ë Œë“œ, ì¬í…Œí¬, ì·¨ì—…êµìœ¡
        - í‚¤ì›Œë“œ: ë¹„íŠ¸ì½”ì¸, ì£¼ì‹, íˆ¬ì ë“± ê¸ˆìœµ ê´€ë ¨

        ì‚½ì… ìˆœì„œ:
        1ìˆœìœ„: êµ¬ê¸€ ì‹œíŠ¸ ìƒí’ˆ DBì—ì„œ ë§¤ì¹­
        2ìˆœìœ„: JSON ê¸°ë°˜ ì¿ íŒ¡ ë§í¬ (í‚¤ì›Œë“œ ë§¤ì¹­)
        3ìˆœìœ„: ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ ë§í¬ (coupang_defaults.json)

        Args:
            content: HTML ë³¸ë¬¸
            keyword: í‚¤ì›Œë“œ
            category_config: ì¹´í…Œê³ ë¦¬ ì„¤ì •
            category_name: ì¹´í…Œê³ ë¦¬ ì´ë¦„ (ê¸°ë³¸ ë§í¬ ì¡°íšŒìš©)

        Returns:
            (ìˆ˜ì •ëœ ì½˜í…ì¸ , ì¿ íŒ¡ ì‚½ì… ì—¬ë¶€) íŠœí”Œ
        """
        # ì¿ íŒ¡ ì œì™¸ ì¡°ê±´ í™•ì¸ (ì¹´í…Œê³ ë¦¬ + í‚¤ì›Œë“œ ê¸°ë°˜) - ê°€ì¥ ë¨¼ì € ì²´í¬
        if self.should_exclude_coupang(keyword, category_name):
            content = content.replace("[COUPANG]", "")
            return content, False

        # ì¿ íŒ¡ì´ í•„ìš”ì—†ëŠ” ì¹´í…Œê³ ë¦¬ë©´ íƒœê·¸ë§Œ ì œê±° (ê¸°ì¡´ ì„¤ì • í˜¸í™˜)
        if not category_config.get("requires_coupang", False):
            content = content.replace("[COUPANG]", "")
            return content, False

        # ì¿ íŒ¡ì´ í•„ìš”í•œ ì¹´í…Œê³ ë¦¬ì¸ë° [COUPANG] íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì½˜í…ì¸  ëì— ì¶”ê°€
        if "[COUPANG]" not in content:
            logger.info("Adding [COUPANG] tag for coupang-required category")
            # ë§ˆë¬´ë¦¬ ì„¹ì…˜ ì•ì— ì¶”ê°€ ì‹œë„
            if "</div>" in content:
                # ë§ˆì§€ë§‰ </div> ì•ì— ì‚½ì…
                last_div = content.rfind("</div>")
                content = content[:last_div] + "\n[COUPANG]\n" + content[last_div:]
            else:
                content += "\n[COUPANG]\n"

        # 1ìˆœìœ„: êµ¬ê¸€ ì‹œíŠ¸ ìƒí’ˆ DB
        try:
            products = get_coupang_products()
            if products:
                matched = match_products_for_content(
                    keyword=keyword,
                    content_summary="",
                    products=products,
                    max_products=2
                )
                if matched:
                    products_html = generate_product_html(matched)
                    content = content.replace("[COUPANG]", products_html)
                    logger.info(f"Google Sheets products inserted: {len(matched)} items")
                    return content, True
        except Exception as e:
            logger.warning(f"Google Sheets product fetch failed: {e}")

        # 2ìˆœìœ„: JSON ê¸°ë°˜ ì¿ íŒ¡ ë§í¬ (í‚¤ì›Œë“œ ë§¤ì¹­)
        coupang = self.get_coupang_link(keyword)
        if coupang:
            button_html = COUPANG_BUTTON_TEMPLATE.format(
                url=coupang["url"],
                button_text=coupang["button_text"]
            )
            content = content.replace("[COUPANG]", button_html)
            logger.info(f"Coupang button inserted: {coupang['button_text']}")
            return content, True

        # 3ìˆœìœ„: ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ ë§í¬
        default_link = self.coupang_defaults.get(category_name)
        if default_link:
            button_html = COUPANG_BUTTON_TEMPLATE.format(
                url=default_link["url"],
                button_text=default_link["text"]
            )
            content = content.replace("[COUPANG]", button_html)
            logger.info(f"Coupang default button inserted: {default_link['text']}")
            return content, True

        # ë§¤ì¹­ ì—†ìŒ - ì¿ íŒ¡ ë§í¬ ì—†ìœ¼ë©´ ë°°ë„ˆë„ ì—†ìŒ
        content = content.replace("[COUPANG]", "")
        logger.info("No coupang link found - tag removed")
        return content, False

    def clean_meta_tags(self, content: str) -> str:
        """ë©”íƒ€ íƒœê·¸ ë° ë‚¨ì€ í”Œë ˆì´ìŠ¤í™€ë” ì •ë¦¬"""
        # [META] íƒœê·¸ ì œê±°
        content = re.sub(r'\[META\].*?\[/META\]', '', content, flags=re.DOTALL)

        # ë‚¨ì€ í”Œë ˆì´ìŠ¤í™€ë” íƒœê·¸ ì œê±°
        content = re.sub(r'\[OFFICIAL_LINK\]', '', content)
        content = re.sub(r'\[COUPANG\]', '', content)
        content = re.sub(r'\[DISCLAIMER\]', '', content)
        content = re.sub(r'\[/?AFFILIATE_NOTICE\]', '', content)  # ì—´ê¸°/ë‹«ê¸° íƒœê·¸ ëª¨ë‘ ì œê±°
        content = re.sub(r'\[IMAGE_\d+[^\]]*\]', '', content)  # [IMAGE_N: ì„¤ëª…] í¬í•¨

        # IMG_CONTEXT ì£¼ì„ ì œê±° (í˜¹ì‹œ ë‚¨ì•„ìˆëŠ” ê²½ìš°)
        content = re.sub(r'<!-- IMG_CONTEXT: .+? -->\s*', '', content)

        return content.strip()

    def _detect_section_type(self, html: str) -> str:
        """ì„¹ì…˜ íƒ€ì… ê°ì§€"""
        html_lower = html.lower().strip()

        if html_lower.startswith('<h1') or html_lower.startswith('<h2') or html_lower.startswith('<h3'):
            return "heading"
        elif '<figure' in html_lower or html_lower.startswith('<img'):
            return "image"
        elif '<ul' in html_lower or '<ol' in html_lower:
            return "list"
        elif '<table' in html_lower:
            return "table"
        elif '<blockquote' in html_lower:
            return "quote"
        else:
            return "paragraph"

    def parse_content_to_sections(self, html: str) -> List[Section]:
        """
        HTML ì½˜í…ì¸ ë¥¼ ì„¹ì…˜ ë°°ì—´ë¡œ ë¶„ë¦¬

        ê° ì„¹ì…˜ì€ ë…ë¦½ì ìœ¼ë¡œ ìˆ˜ì • ê°€ëŠ¥í•œ ë‹¨ìœ„
        """
        sections = []

        # ìµœìƒìœ„ HTML íƒœê·¸ë“¤ì„ ë§¤ì¹­
        # h1-h6, p, div, figure, ul, ol, table, blockquote, section
        pattern = r'(<(?:h[1-6]|p|div|figure|ul|ol|table|blockquote|section)[^>]*>.*?</(?:h[1-6]|p|div|figure|ul|ol|table|blockquote|section)>)'

        matches = re.findall(pattern, html, re.DOTALL | re.IGNORECASE)

        if matches:
            for i, match in enumerate(matches):
                section_html = match.strip()
                if not section_html:
                    continue

                # ë¹ˆ ì½˜í…ì¸  ì œì™¸ (ì´ë¯¸ì§€ëŠ” ì˜ˆì™¸)
                text_content = re.sub(r'<[^>]+>', '', section_html).strip()
                if not text_content and '<img' not in section_html.lower() and '<figure' not in section_html.lower():
                    continue

                section_type = self._detect_section_type(section_html)
                sections.append(Section(
                    id=f"section-{uuid.uuid4().hex[:8]}",
                    index=len(sections),
                    type=section_type,
                    html=section_html
                ))
        else:
            # ë§¤ì¹­ ì•ˆ ëœ ê²½ìš° ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì„¹ì…˜ìœ¼ë¡œ
            if html.strip():
                sections.append(Section(
                    id=f"section-{uuid.uuid4().hex[:8]}",
                    index=0,
                    type="paragraph",
                    html=html.strip()
                ))

        logger.info(f"Parsed {len(sections)} sections from content")
        return sections

    def generate_full_post(
        self,
        keyword: str,
        news_data: str = "",
        custom_context: str = None,
        force_category: str = None
    ) -> GeneratedPost:
        """
        ì¹´í…Œê³ ë¦¬ë³„ ì „ì²´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„±

        Args:
            keyword: í‚¤ì›Œë“œ
            news_data: ë‰´ìŠ¤ ìš”ì•½ ë°ì´í„°
            custom_context: ì‚¬ìš©ì ì§€ì • ì‘ì„± ë°©í–¥ (ì§ì ‘ ì‘ì„± ëª¨ë“œ)
            force_category: ê°•ì œ ì¹´í…Œê³ ë¦¬ ì§€ì • (ì§ì ‘ ì‘ì„± ëª¨ë“œ)

        Returns:
            GeneratedPost ê°ì²´
        """
        print("\n" + "=" * 60)
        print("ğŸ“ ë¸”ë¡œê·¸ ê¸€ ìƒì„± í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        if custom_context:
            print("   [ì§ì ‘ ì‘ì„± ëª¨ë“œ]")
        print("=" * 60)

        # Step 1: í‚¤ì›Œë“œ ë¶„ì„ ë° ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        print(f"\n[Step 1/8] í‚¤ì›Œë“œ ë¶„ì„")
        print(f"  â””â”€ í‚¤ì›Œë“œ: {keyword}")

        # ì§ì ‘ ì‘ì„± ëª¨ë“œì—ì„œëŠ” ì‚¬ìš©ì ì§€ì • ì¹´í…Œê³ ë¦¬ ìš°ì„ 
        if force_category:
            category_name = force_category
            category_config = self.categories_config.get("categories", {}).get(force_category, {})
            if not category_config:
                # ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
                category_config = {"template": "trend", "requires_coupang": False}
            print(f"  â””â”€ ì¹´í…Œê³ ë¦¬: {category_name} (ì‚¬ìš©ì ì§€ì •)")
        else:
            category_name, category_config = self.classify_category(keyword)
            print(f"  â””â”€ ì¹´í…Œê³ ë¦¬: {category_name}")

        template_name = category_config.get("template", "trend")
        is_evergreen = self.is_evergreen_keyword(keyword)
        print(f"  â””â”€ ì—ë²„ê·¸ë¦°: {'âœ… Yes' if is_evergreen else 'âŒ No'}")
        print(f"  â””â”€ í…œí”Œë¦¿: {template_name}")

        # Step 1.5: íŠ¸ë Œë“œ ë§¥ë½ ìˆ˜ì§‘ ë˜ëŠ” ì‚¬ìš©ì ì§€ì • ë§¥ë½ ì‚¬ìš©
        print(f"\n[Step 1.5/8] íŠ¸ë Œë“œ ë§¥ë½ ìˆ˜ì§‘")
        trend_context = ""

        if custom_context:
            # ì§ì ‘ ì‘ì„± ëª¨ë“œ: ì‚¬ìš©ì ì…ë ¥ì„ íŠ¸ë Œë“œ ë§¥ë½ìœ¼ë¡œ ì‚¬ìš©
            trend_context = f"""
[ì‘ì„± ë°©í–¥ - ì‚¬ìš©ì ìš”ì²­]
{custom_context}

ì¤‘ìš”: ìœ„ ì‘ì„± ë°©í–¥ì— ë§ì¶°ì„œ ê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
ì‚¬ìš©ìê°€ ìš”ì²­í•œ í†¤, ìŠ¤íƒ€ì¼, í¬í•¨í•  ë‚´ìš©ì„ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì„¸ìš”.
"""
            print(f"  âœ… ì‚¬ìš©ì ì§€ì • ì‘ì„± ë°©í–¥ ì ìš©")
            print(f"     {custom_context[:80]}...")
        elif category_name == "íŠ¸ë Œë“œ" or not is_evergreen:
            trend_context = self.get_trend_context(keyword)
            if trend_context:
                print(f"  âœ… íŠ¸ë Œë“œ ë§¥ë½ ìˆ˜ì§‘ ì™„ë£Œ (ë‰´ìŠ¤ ê¸°ë°˜)")
            else:
                print(f"  âš ï¸ íŠ¸ë Œë“œ ë§¥ë½ ì—†ìŒ")
        else:
            print(f"  â„¹ï¸ ì—ë²„ê·¸ë¦° í‚¤ì›Œë“œ - íŠ¸ë Œë“œ ë§¥ë½ ìŠ¤í‚µ")

        # ì´ë¯¸ì§€ ì¤‘ë³µ ë°©ì§€ ì´ˆê¸°í™”
        self.image_fetcher.reset_used_images()

        # Step 2: ì›¹ê²€ìƒ‰ (íŠ¸ë Œë“œ + ì—ë²„ê·¸ë¦° ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ì ìš©)
        print(f"\n[Step 2/8] ì›¹ê²€ìƒ‰ ì‹¤í–‰")

        # ì›¹ ê²€ìƒ‰ ì ìš© ì¹´í…Œê³ ë¦¬ (íŠ¸ë Œë“œ + ì—ë²„ê·¸ë¦°)
        web_search_categories = ["íŠ¸ë Œë“œ", "ì—°ì˜ˆ", "ìƒí™œì •ë³´", "ì¬í…Œí¬", "ê±´ê°•", "IT/í…Œí¬", "ì·¨ì—…êµìœ¡"]

        if category_name in web_search_categories:
            print(f"  ğŸ” ì›¹ ê²€ìƒ‰ ìˆ˜í–‰: {keyword} (ì¹´í…Œê³ ë¦¬: {category_name})")
            web_data = self.perform_web_search(keyword)

            if web_data and web_data.get("content"):
                print(f"  âœ… ì›¹ ê²€ìƒ‰ ì •ë³´ ì·¨í•© ì™„ë£Œ ({len(web_data.get('content', ''))}ì)")
            else:
                print(f"  âš ï¸ ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ, AI ê¸°ë³¸ ì§€ì‹ìœ¼ë¡œ ì‘ì„±")
        else:
            print(f"  â„¹ï¸ ì›¹ ê²€ìƒ‰ ìŠ¤í‚µ (ì¹´í…Œê³ ë¦¬: {category_name})")
            web_data = {"sources": [], "content": ""}

        sources = web_data.get("sources", []) if web_data else []
        if sources:
            print(f"  â””â”€ ê²€ìƒ‰ ê²°ê³¼: {len(sources)}ê°œ ì¶œì²˜")
            for src in sources[:3]:
                print(f"      â€¢ {src['title'][:40]}...")
        elif category_name in web_search_categories:
            print(f"  â””â”€ ê²€ìƒ‰ ê²°ê³¼: ì—†ìŒ")

        # Step 2.5a: ë¸”ë¡œê·¸ ì°¸ì¡° ë¶„ì„
        print(f"\n[Step 2.5a/8] ë¸”ë¡œê·¸ ì°¸ì¡° ë¶„ì„")
        blog_analysis = ""
        try:
            from crawlers.blog_reference import BlogReferenceCrawler
            blog_ref = BlogReferenceCrawler()
            blog_analysis = blog_ref.get_blog_analysis(keyword, count=3)
            if blog_analysis:
                print(f"  âœ… ë¸”ë¡œê·¸ ì°¸ì¡° ë¶„ì„ ì™„ë£Œ")
                # trend_contextì— ë¸”ë¡œê·¸ ë¶„ì„ ì¶”ê°€
                if not trend_context:
                    trend_context = ""
                trend_context += f"\n\n[ì°¸ê³  ë¸”ë¡œê·¸ êµ¬ì¡° ë¶„ì„]\n{blog_analysis}\nìœ„ ì¸ê¸° ë¸”ë¡œê·¸ì˜ êµ¬ì¡°ì™€ ì†Œì œëª© íŒ¨í„´ì„ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”."
            else:
                print(f"  âš ï¸ ë¸”ë¡œê·¸ ì°¸ì¡° ê²°ê³¼ ì—†ìŒ")
        except Exception as e:
            logger.warning(f"Blog reference failed: {e}")
            print(f"  âš ï¸ ë¸”ë¡œê·¸ ì°¸ì¡° ì‹¤íŒ¨: {e}")

        # Step 2.5b: ì¸ë¬¼ í‚¤ì›Œë“œ ê°ì§€ (ì œëª© ìƒì„± ì „ì— í•„ìš”)
        is_person = is_person_keyword(keyword)
        if is_person:
            print(f"\n  ğŸ‘¤ ì¸ë¬¼ í‚¤ì›Œë“œ ê°ì§€: {keyword}")

        # Step 3: ì œëª© ìƒì„±
        print(f"\n[Step 3/8] ì œëª© ìƒì„±")
        print(f"  â””â”€ Claude API í˜¸ì¶œ ì¤‘...")
        # ì¸ë¬¼ í‚¤ì›Œë“œë©´ ë‰´ìŠ¤ ë°ì´í„° ì „ë‹¬í•˜ì—¬ íŒ©íŠ¸ ê¸°ë°˜ ì œëª© ìƒì„±
        web_content_for_title = web_data.get("content", "")[:800] if web_data else ""
        title = self.generate_title(keyword, news_data=web_content_for_title, is_person=is_person)
        print(f"  â””â”€ ìƒì„±ëœ ì œëª©: {title}")

        # Step 4: ë³¸ë¬¸ ìƒì„± (í…œí”Œë¦¿ ë‹¤ì–‘í™” ì‹œìŠ¤í…œ + íŠ¸ë Œë“œ ë§¥ë½)
        print(f"\n[Step 4/8] ë³¸ë¬¸ ìƒì„± (í…œí”Œë¦¿ ë‹¤ì–‘í™”)")
        print(f"  â””â”€ ì—ë²„ê·¸ë¦°: {'âœ… Yes' if is_evergreen else 'âŒ No'}")
        if trend_context:
            print(f"  â””â”€ íŠ¸ë Œë“œ ë§¥ë½: í¬í•¨")
        print(f"  â””â”€ Claude API í˜¸ì¶œ ì¤‘...")
        content, content_sources, template_info = self.generate_content_with_template(
            keyword, news_data, template_name,
            category_name=category_name,
            is_evergreen=is_evergreen,
            web_data=web_data,
            trend_context=trend_context  # íŠ¸ë Œë“œ ë§¥ë½ ì¶”ê°€
        )
        print(f"  â””â”€ ìƒì„± ì™„ë£Œ: {len(content)} chars")
        print(f"  â””â”€ ì‚¬ìš©ëœ í…œí”Œë¦¿: {template_info['name']} ({template_info['key']})")
        print(f"  â””â”€ ëª©í‘œ ê¸€ììˆ˜: {template_info['word_count']}ì, ì´ë¯¸ì§€: {template_info['image_count']}ê°œ")
        sources = content_sources if content_sources else sources

        # Step 5: ë©”íƒ€ ì„¤ëª… ì¶”ì¶œ
        excerpt = self._extract_meta_description(content)
        if not excerpt:
            excerpt = f"{keyword}ì— ëŒ€í•œ ì™„ë²½ ê°€ì´ë“œ! í•µì‹¬ ì •ë³´ë¶€í„° ì‹¤ì „ íŒê¹Œì§€ í•œ ë²ˆì— ì•Œì•„ë³´ì„¸ìš”."[:160]

        # Step 5: í›„ì²˜ë¦¬ (ì´ë¯¸ì§€, ë§í¬, ì¿ íŒ¡)
        print(f"\n[Step 5/8] í›„ì²˜ë¦¬")

        # ì´ë¯¸ì§€ ì‚½ì… (í…œí”Œë¦¿ì—ì„œ ì§€ì •í•œ ì´ë¯¸ì§€ ê°œìˆ˜ ì‚¬ìš©)
        image_count = template_info.get('image_count', 4)
        content = self.insert_images(content, keyword, category_name, image_count)
        print(f"  â””â”€ ì´ë¯¸ì§€ ì‚½ì… ì™„ë£Œ")

        # ê´€ë ¨ ì‚¬ì´íŠ¸ ë§í¬ ìë™ ì‚½ì… (ì¹´í…Œê³ ë¦¬ ìƒê´€ì—†ì´ í•­ìƒ)
        print("  ğŸ”— ê´€ë ¨ ì‚¬ì´íŠ¸ ë§í¬ ì‚½ì… ì¤‘...")
        content = insert_related_links(content, keyword)
        print("  âœ… ë§í¬ ì‚½ì… ì™„ë£Œ")

        # ê±´ê°• ë©´ì±…ë¬¸êµ¬ ì‚½ì…
        if category_config.get("requires_disclaimer", False):
            content = self.insert_disclaimer(content)
            print(f"  â””â”€ ê±´ê°• ë©´ì±…ë¬¸êµ¬ ì‚½ì… ì™„ë£Œ")
        else:
            content = content.replace("[DISCLAIMER]", "")

        # Step 6: ì¿ íŒ¡ ì²˜ë¦¬
        print(f"\n[Step 6/8] ì¿ íŒ¡ ì²˜ë¦¬")
        content, has_coupang = self.insert_coupang_products(
            content, keyword, category_config, category_name
        )
        print(f"  â””â”€ ì¿ íŒ¡ ì‚½ì…: {'âœ… Yes' if has_coupang else 'âŒ No'}")

        # íŒŒíŠ¸ë„ˆìŠ¤ ë¬¸êµ¬ ì‚½ì…
        content = self.insert_affiliate_notice(content, has_coupang)

        # ì¹´í…Œê³ ë¦¬ ë±ƒì§€ ì‚½ì…
        content = self.insert_category_badge(content, category_name)

        # ì •ë¦¬
        content = self.clean_meta_tags(content)

        # Step 7: ì„¹ì…˜ ë¶„ë¦¬
        print(f"\n[Step 7/8] ì„¹ì…˜ ë¶„ë¦¬")
        sections = self.parse_content_to_sections(content)
        print(f"  â””â”€ ì„¹ì…˜ ìˆ˜: {len(sections)}ê°œ")
        for s in sections[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
            text_preview = re.sub(r'<[^>]+>', '', s.html)[:30].strip()
            print(f"      â€¢ [{s.type}] {text_preview}...")

        # Step 8: ìµœì¢… ê²°ê³¼
        print(f"\n[Step 8/8] ìµœì¢… ê²°ê³¼")
        print(f"  â””â”€ ì œëª©: {title}")
        print(f"  â””â”€ ì¹´í…Œê³ ë¦¬: {category_name}")
        print(f"  â””â”€ ì½˜í…ì¸  ê¸¸ì´: {len(content)} chars")
        print(f"  â””â”€ ì„¹ì…˜ ìˆ˜: {len(sections)}ê°œ")
        print(f"  â””â”€ ì›¹ ì¶œì²˜: {len(sources)}ê°œ")
        print(f"  â””â”€ ì¿ íŒ¡: {'ìˆìŒ' if has_coupang else 'ì—†ìŒ'}")

        print("\n" + "=" * 60)
        print("âœ… ë¸”ë¡œê·¸ ê¸€ ìƒì„± ì™„ë£Œ!")
        print("=" * 60 + "\n")

        # ê¸°ì¡´ ë¡œê±° í˜¸ì¶œ (íŒŒì¼ ë¡œê·¸ìš©)
        logger.info(f"Post generation complete: {title}")
        logger.info(f"  Category: {category_name}, Template: {template_name}")
        logger.info(f"  Content: {len(content)} chars, Sections: {len(sections)}, Sources: {len(sources)}, Coupang: {has_coupang}")

        return GeneratedPost(
            title=title,
            content=content,
            excerpt=excerpt,
            category=category_name,
            template=template_name,
            has_coupang=has_coupang,
            sources=sources,
            sections=sections
        )


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    logging.basicConfig(level=logging.INFO)
    generator = ContentGenerator()

    # í…ŒìŠ¤íŠ¸ í‚¤ì›Œë“œë“¤
    test_keywords = [
        ("ì—°ë§ì •ì‚°", "ì¬í…Œí¬"),
        ("ì•„ì´í°16", "ITí…Œí¬"),
        ("BTS ì»´ë°±", "ì—°ì˜ˆ"),
        ("ë‹¤ì´ì–´íŠ¸", "ê±´ê°•"),
    ]

    for keyword, expected_category in test_keywords[:1]:  # ì²« ë²ˆì§¸ë§Œ í…ŒìŠ¤íŠ¸
        print(f"\n{'='*60}")
        print(f"Testing: {keyword} (expected: {expected_category})")
        print(f"{'='*60}")

        post = generator.generate_full_post(keyword)

        print(f"\nResult:")
        print(f"  Title: {post.title}")
        print(f"  Category: {post.category}")
        print(f"  Template: {post.template}")
        print(f"  Excerpt: {post.excerpt}")
        print(f"  Has Coupang: {post.has_coupang}")
        print(f"  Content length: {len(post.content)} chars")
        print(f"\nContent preview:\n{post.content[:1500]}...")
